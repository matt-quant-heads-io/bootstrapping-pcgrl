{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e2b3b1e-9d00-4164-b0cb-9a48934702b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get install -y \\\n",
    "#     libgl1-mesa-dev \\\n",
    "#     libgl1-mesa-glx \\\n",
    "#     libglew-dev \\\n",
    "#     libosmesa6-dev \\\n",
    "#     software-properties-common \\\n",
    "#     patchelf \\\n",
    "#     xvfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a57d148-4143-4bbc-8326-4565c6220e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym==0.21.0\n",
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install xvfbwrapper\n",
    "# !pip install huggingface_hub\n",
    "# pip install gym==0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a47e4d-1274-4222-8796-39adb855a079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-msiper/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments, DecisionTransformerGPT2Model\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d82450b4-a8c9-4188-9954-ec6d7703627b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 389\u001b[0m\n\u001b[1;32m    387\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# print(dataset)\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m collator \u001b[38;5;241m=\u001b[39m \u001b[43mDecisionTransformerGymDataCollator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m config \u001b[38;5;241m=\u001b[39m DecisionTransformerConfig(state_dim\u001b[38;5;241m=\u001b[39mcollator\u001b[38;5;241m.\u001b[39mstate_dim, act_dim\u001b[38;5;241m=\u001b[39mcollator\u001b[38;5;241m.\u001b[39mact_dim)\n\u001b[1;32m    393\u001b[0m config\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m768\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mDecisionTransformerGymDataCollator.__init__\u001b[0;34m(self, dataset, gamma, epsilon)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m, in \u001b[0;36mDecisionTransformerGymDataCollator.initialize_variables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m obs_buffer \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m trajectories_length \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobservations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     21\u001b[0m     obs_buffer\u001b[38;5;241m.\u001b[39mappend(obs)\n\u001b[1;32m     22\u001b[0m     trajectories_length\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(obs))\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class DecisionTransformerGymDataCollator:\n",
    "    def __init__(self, dataset, gamma=1.0, epsilon=1e-5):\n",
    "        self.dataset = dataset\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.max_length = 10\n",
    "        self.max_ep_len = 1000\n",
    "        self.state_size = 484*8\n",
    "        self.action_size = 8\n",
    "        self.reward_scale = 1.0\n",
    "\n",
    "        self.initialize_variables()\n",
    "\n",
    "\n",
    "    def initialize_variables(self):\n",
    "        obs_buffer = []\n",
    "        trajectories_length = []\n",
    "        for obs in self.dataset[\"observations\"]:\n",
    "            obs_buffer.append(obs)\n",
    "            trajectories_length.append(len(obs))\n",
    "        obs_buffer = np.vstack(obs_buffer)\n",
    "\n",
    "        self.num_trajectories = len(trajectories_length)\n",
    "        self.mean = np.mean(obs_buffer, axis=0)\n",
    "        self.std = np.std(obs_buffer, axis=0) + self.epsilon\n",
    "        self.trajectories_prob = np.array(trajectories_length) / sum(trajectories_length)\n",
    "\n",
    "    \n",
    "    def calculate_returns(self, x):\n",
    "        l = x.shape[0]\n",
    "        gammas = np.logspace(l-1, 0, num=l, base=self.gamma, endpoint=True)\n",
    "        rets = x * gammas\n",
    "        rets = np.cumsum(rets[::-1])[::-1]\n",
    "        return rets\n",
    "\n",
    "\n",
    "    def __call__(self, batch_size):\n",
    "        batch_idxs = np.random.choice(\n",
    "            self.num_trajectories,\n",
    "            size=batch_size,\n",
    "            replace=True,\n",
    "            p=self.trajectories_prob,\n",
    "        ).tolist()\n",
    "\n",
    "        R, s, a, r, t, d, mask = [], [], [], [], [], [], []\n",
    "\n",
    "        for idx in batch_idxs:\n",
    "            T = self.dataset[idx]\n",
    "            start = random.randint(0, len(T[\"dones\"]) - 1)\n",
    "\n",
    "            states = np.array(T[\"observations\"][start : start + self.max_length]).reshape(1, -1, self.state_size)\n",
    "            seq_length = states.shape[1]\n",
    "            padd_states = np.zeros((1, self.max_length - seq_length, self.state_size))\n",
    "            states = np.concatenate([padd_states, states], axis=1)\n",
    "            states = (states - self.mean) / self.std\n",
    "            s.append(states)\n",
    "\n",
    "            actions = np.array(T[\"actions\"][start : start + self.max_length]).reshape(1, -1, self.action_size)\n",
    "            padd_action = np.ones((1, self.max_length - seq_length, self.action_size)) * -10.0\n",
    "            actions = np.concatenate([padd_action, actions], axis=1)\n",
    "            a.append(actions)\n",
    "\n",
    "            rewards = np.array(T[\"rewards\"][start : start + self.max_length]).reshape(1, -1, 1)\n",
    "            padd_rewards = np.zeros((1, self.max_length - seq_length, 1))\n",
    "            rewards = np.concatenate([padd_rewards, rewards], axis=1)\n",
    "            r.append(rewards)\n",
    "\n",
    "            dones = np.array(T[\"dones\"][start : start + self.max_length]).reshape(1, -1)\n",
    "            padd_dones = np.ones((1, self.max_length - seq_length)) * 2\n",
    "            dones = np.concatenate([padd_dones, dones], axis=1)\n",
    "            d.append(dones)\n",
    "\n",
    "            timesteps = np.arange(start, start + seq_length).reshape(1, -1)\n",
    "            timesteps[timesteps >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
    "            timesteps = np.concatenate([np.zeros((1, self.max_length - seq_length)), timesteps], axis=1)\n",
    "            t.append(timesteps)\n",
    "\n",
    "            returns = self.calculate_returns(np.array(T[\"rewards\"][start:]))\n",
    "            returns = returns[:seq_length].reshape(1, -1, 1)\n",
    "            if returns.shape[1] < seq_length:\n",
    "                returns = np.concatenate([returns, np.zeros((1, 1, 1))], axis=1)\n",
    "            padd_returns = np.zeros((1, self.max_length - seq_length, 1))\n",
    "            returns = np.concatenate([padd_returns, returns], axis=1)\n",
    "            R.append(returns / self.reward_scale)\n",
    "            \n",
    "            mask.append(\n",
    "                np.concatenate([np.zeros((1, self.max_length - seq_length)), np.ones((1, seq_length))], axis=1)\n",
    "            )\n",
    "\n",
    "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
    "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
    "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
    "        R = torch.from_numpy(np.concatenate(R, axis=0)).float()\n",
    "        t = torch.from_numpy(np.concatenate(t, axis=0)).long()\n",
    "        d = torch.from_numpy(np.concatenate(d, axis=0))\n",
    "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
    "\n",
    "        return {\n",
    "            \"returns_to_go\": R,\n",
    "            \"states\": s,\n",
    "            \"actions\": a,\n",
    "            \"rewards\": r,\n",
    "            \"timesteps\": t,\n",
    "            # \"dones\": d,\n",
    "            \"attention_mask\": mask,\n",
    "        }\n",
    "\n",
    "# @dataclass\n",
    "# class DecisionTransformerGymDataCollator:\n",
    "#     return_tensors: str = \"pt\"\n",
    "#     max_len: int = 10 #subsets of the episode we use for training\n",
    "#     state_dim: int = 484*8  # size of state space\n",
    "#     act_dim: int = 8  # size of action space\n",
    "#     max_ep_len: int = 1000 # max episode length in the dataset\n",
    "#     scale: float = 1.0  # normalization of rewards/returns\n",
    "#     state_mean: np.array = None  # to store state means\n",
    "#     state_std: np.array = None  # to store state stds\n",
    "#     p_sample: np.array = None  # a distribution to take account trajectory lengths\n",
    "#     n_traj: int = 0 # to store the number of trajectories in the dataset\n",
    "\n",
    "#     def __init__(self, dataset) -> None:\n",
    "#         self.dataset = dataset\n",
    "#         self.act_dim = len(self.dataset[0][\"actions\"][0])\n",
    "#         self.state_dim = len(self.dataset[0][\"observations\"][0])\n",
    "        \n",
    "#         # calculate dataset stats for normalization of states\n",
    "#         states = []\n",
    "#         traj_lens = []\n",
    "#         for i, d in enumerate(dataset):\n",
    "#             obs = d[\"observations\"]\n",
    "#             states.extend(obs)\n",
    "#             traj_lens.append(len(obs))\n",
    "#         self.n_traj = len(traj_lens)\n",
    "#         print(f\"self.n_traj: {self.n_traj}\")\n",
    "#         states = np.vstack(states)\n",
    "#         self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "        \n",
    "#         traj_lens = np.array(traj_lens)\n",
    "#         self.p_sample = traj_lens / sum(traj_lens)\n",
    "#         self._inds_used_in_training = []\n",
    "\n",
    "#     def _discount_cumsum(self, x, gamma):\n",
    "#         discount_cumsum = np.zeros_like(x)\n",
    "#         discount_cumsum[-1] = x[-1]\n",
    "#         for t in reversed(range(x.shape[0] - 1)):\n",
    "#             discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "#         return discount_cumsum\n",
    "\n",
    "#     def __call__(self, features):\n",
    "#         batch_size = len(features)\n",
    "#         # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
    "#         batch_inds = np.random.choice(\n",
    "#             np.arange(self.n_traj),\n",
    "#             size=batch_size,\n",
    "#             replace=True,\n",
    "#             p=self.p_sample,  # reweights so we sample according to timesteps\n",
    "#         )\n",
    "#         open(\"/home/jupyter-msiper/bootstrapping-pcgrl/inds_used.txt\", \"w\").write('\\n'.join([str(i) for i in batch_inds]))\n",
    "#         # a batch of dataset features\n",
    "#         s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
    "        \n",
    "#         for ind in batch_inds:\n",
    "#             # for feature in features:\n",
    "#             self._inds_used_in_training.append(int(ind))\n",
    "#             feature = self.dataset[int(ind)]\n",
    "#             si = random.randint(0, len(feature[\"rewards\"]) - 1)\n",
    "#             # si = 0\n",
    "\n",
    "#             # get sequences from dataset\n",
    "#             s.append(np.array(feature[\"observations\"][si : si + self.max_len]).reshape(1, -1, self.state_dim))\n",
    "#             a.append(np.array(feature[\"actions\"][si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
    "#             r.append(np.array(feature[\"rewards\"][si : si + self.max_len]).reshape(1, -1, 1))\n",
    "\n",
    "#             d.append(np.array(feature[\"dones\"][si : si + self.max_len]).reshape(1, -1))\n",
    "#             timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "#             timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
    "#             rtg.append(\n",
    "#                 self._discount_cumsum(np.array(feature[\"rewards\"][si:]), gamma=1.0)[\n",
    "#                     : s[-1].shape[1]   # TODO check the +1 removed here\n",
    "#                 ].reshape(1, -1, 1)\n",
    "#             )\n",
    "#             if rtg[-1].shape[1] < s[-1].shape[1]:\n",
    "#                 print(\"if true\")\n",
    "#                 rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "#             # padding and state + reward normalization\n",
    "#             tlen = s[-1].shape[1]\n",
    "#             s[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, self.state_dim)), s[-1]], axis=1)\n",
    "#             # s[-1] = (s[-1] - self.state_mean) / self.state_std\n",
    "#             # a[-1] = np.concatenate(\n",
    "#             #     [np.ones((1, self.max_len - tlen, self.act_dim)) * -10.0, a[-1]],\n",
    "#             #     axis=1,\n",
    "#             # )\n",
    "#             a[-1] = np.concatenate(\n",
    "#                 [np.ones((1, self.max_len - tlen, self.act_dim)), a[-1]],\n",
    "#                 axis=1,\n",
    "#             )\n",
    "#             r[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), r[-1]], axis=1)\n",
    "#             d[-1] = np.concatenate([np.ones((1, self.max_len - tlen)) * 2, d[-1]], axis=1)\n",
    "#             rtg[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), rtg[-1]], axis=1) / self.scale\n",
    "#             timesteps[-1] = np.concatenate([np.zeros((1, self.max_len - tlen)), timesteps[-1]], axis=1)\n",
    "#             mask.append(np.concatenate([np.zeros((1, self.max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
    "\n",
    "#         s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
    "#         a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
    "#         r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
    "#         d = torch.from_numpy(np.concatenate(d, axis=0))\n",
    "#         rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
    "#         timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
    "#         mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
    "\n",
    "#         return {\n",
    "#             \"states\": s,\n",
    "#             \"actions\": a,\n",
    "#             \"rewards\": r,\n",
    "#             \"returns_to_go\": rtg,\n",
    "#             \"timesteps\": timesteps,\n",
    "#             \"attention_mask\": mask,\n",
    "#         }\n",
    "    \n",
    "    \n",
    "# class TrainableDT(DecisionTransformerModel):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "\n",
    "#     def forward(self, **kwargs):\n",
    "#         output = super().forward(**kwargs)\n",
    "#         # add the DT loss\n",
    "#         action_preds = output[1]\n",
    "#         action_targets = kwargs[\"actions\"]\n",
    "#         attention_mask = kwargs[\"attention_mask\"]\n",
    "#         act_dim = action_preds.shape[2]\n",
    "#         action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "#         action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        \n",
    "#         # pred_indices = action_preds.argmax(dim=-1)\n",
    "#         # target_indices = action_targets.argmax(dim=-1)\n",
    "\n",
    "#         # loss = torch.mean((action_preds - action_targets) ** 2)\n",
    "#         loss = nn.CrossEntropyLoss()(action_preds, action_targets)\n",
    "\n",
    "#         return {\"loss\": loss}\n",
    "\n",
    "#     def original_forward(self, **kwargs):\n",
    "#         return super().forward(**kwargs)\n",
    "    \n",
    "    \n",
    "class TrainableDT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.action_size = 8\n",
    "        self.state_size = 484*8\n",
    "        self.initializer_range = 0.02\n",
    "        self.hidden_size = 768\n",
    "\n",
    "        # config = DecisionTransformerConfig(state_dim=self.state_size, act_dim=self.action_size)\n",
    "        self.transformer = DecisionTransformerGPT2Model(config)\n",
    "        # config.max_ep_len = 4096\n",
    "        self.embed_t = nn.Embedding(config.max_ep_len, self.hidden_size)\n",
    "        self.embed_s = torch.nn.Linear(self.state_size, self.hidden_size)\n",
    "        self.embed_a = torch.nn.Linear(self.action_size, self.hidden_size)\n",
    "        self.embed_R = torch.nn.Linear(1, self.hidden_size)\n",
    "\n",
    "        self.embed_layer_norm = nn.LayerNorm(self.hidden_size)\n",
    "\n",
    "        self.predict_state = torch.nn.Linear(self.hidden_size, self.state_size)\n",
    "        self.predict_return = torch.nn.Linear(self.hidden_size, 1)\n",
    "        self.predict_action = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, config.act_dim),\n",
    "            # nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "    def pre_forward(self,\n",
    "                returns_to_go=None,\n",
    "                states=None,\n",
    "                actions=None,\n",
    "                timesteps=None,\n",
    "                rewards=None,\n",
    "                attention_mask=None,\n",
    "                output_hidden_states=False,\n",
    "                output_attentions=False):\n",
    "        \n",
    "        batch_size, seq_length = states.shape[0], states.shape[1]\n",
    "\n",
    "        attention_mask = torch.ones((batch_size, seq_length), dtype=torch.long).to(states.device)\n",
    "\n",
    "        # Embed each modality with a different head\n",
    "        pos_embedding = self.embed_t(timesteps)\n",
    "        s_embeddings = self.embed_s(states) + pos_embedding\n",
    "        a_embeddings = self.embed_a(actions) + pos_embedding\n",
    "        R_embeddings = self.embed_R(returns_to_go) + pos_embedding\n",
    "\n",
    "        # (R_1, s_1, a_1, R_2, s_2, a_2, ...) Sequence\n",
    "        input_embeds = (\n",
    "            torch.stack((R_embeddings, s_embeddings, a_embeddings), dim=1)\n",
    "            .permute(0, 2, 1, 3)\n",
    "            .reshape(batch_size, 3 * seq_length, self.hidden_size)\n",
    "        )\n",
    "        input_embeds = self.embed_layer_norm(input_embeds)\n",
    "\n",
    "        # Corresponding Attention Stack\n",
    "        input_attention_masks = (\n",
    "            torch.stack((attention_mask, attention_mask, attention_mask), dim=1)\n",
    "            .permute(0, 2, 1)\n",
    "            .reshape(batch_size, 3 * seq_length)\n",
    "        )\n",
    "        \n",
    "        # we feed in the input embeddings (not word indices as in NLP) to the model\n",
    "        transformer_outputs = self.transformer(\n",
    "            inputs_embeds=input_embeds,\n",
    "            attention_mask=input_attention_masks,\n",
    "            position_ids=torch.zeros(input_attention_masks.shape, \n",
    "                                     device=states.device, \n",
    "                                     dtype=torch.long),\n",
    "            output_attentions=True,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        x = transformer_outputs[\"last_hidden_state\"]\n",
    "\n",
    "        # reshape x so that the second dimension corresponds to the original\n",
    "        # returns (0), states (1), or actions (2); i.e. x[:,1,t] is the token for s_t\n",
    "        x = x.reshape(batch_size, seq_length, 3, self.hidden_size).permute(0, 2, 1, 3)\n",
    "\n",
    "        # get predictions\n",
    "        return_preds = self.predict_return(x[:, 2])  # predict next return given state and action\n",
    "        state_preds = self.predict_state(x[:, 1])  # predict next state given state and action\n",
    "        action_preds = self.predict_action(x[:, 0])  # predict next action given state\n",
    "\n",
    "        return {\n",
    "            \"state_preds\": state_preds,\n",
    "            \"action_preds\": action_preds,\n",
    "            \"return_preds\": return_preds,\n",
    "            \"last_hidden_state\": transformer_outputs.last_hidden_state,\n",
    "            \"hidden_states\": transformer_outputs.hidden_states,\n",
    "            \"attentions\": transformer_outputs.attentions,\n",
    "        }\n",
    "\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        output = self.pre_forward(**kwargs)\n",
    "        # add the DT loss\n",
    "        print(f'output[\"attentions\"]: {output[\"attentions\"][0].shape} {output[\"attentions\"][1].shape} {output[\"attentions\"][2].shape}')\n",
    "        print(f'action_preds: {output[\"action_preds\"].shape}')\n",
    "        action_preds = output[\"action_preds\"]\n",
    "        action_targets = kwargs[\"actions\"]\n",
    "        attention_mask = output[\"attentions\"][2]\n",
    "        act_dim = action_preds.shape[2]\n",
    "        action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        \n",
    "        # pred_indices = action_preds.argmax(dim=-1)\n",
    "        # target_indices = action_targets.argmax(dim=-1)\n",
    "\n",
    "        # loss = torch.mean((action_preds - action_targets) ** 2)\n",
    "        loss = nn.CrossEntropyLoss()(action_preds, action_targets)\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "\n",
    "# data = np.load(\"/home/jupyter-msiper/bootstrapping-pcgrl/raw_dataset_data_lg.npz\")\n",
    "# dataset = Dataset.from_dict({item: data[item] for item in data.files})\n",
    "\n",
    "# Check here you can load the pkl dataset bootstrapping-pcgrl/raw_dataset_data_lg_temp.pkl and train\n",
    "dataset = None\n",
    "with open('/home/jupyter-msiper/bootstrapping-pcgrl/raw_dataset_dt_lg_10K_episodes.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "# print(dataset)\n",
    "collator = DecisionTransformerGymDataCollator(dataset)\n",
    "\n",
    "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
    "\n",
    "config.hidden_size = 768\n",
    "config.n_head = 3\n",
    "# config.scale_attn_by_inverse_layer_idx = False\n",
    "# config.embd_pdrop = 0.0\n",
    "# config.attn_pdrop = 0.0\n",
    "# config.action_tanh = False\n",
    "model = TrainableDT(config)\n",
    "print(config)\n",
    "collator._inds_used_in_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506d43d5-7aae-42b0-87bc-c3470e24ed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f25e7a-c445-44ea-92e9-11db2c6f7390",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output2/\",\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=10000,\n",
    "    per_device_train_batch_size=1,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.01,\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=0.25,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af5fa43-8419-4c74-92d5-29d48953e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEXT STEPS:\n",
    "# 1) Implement inference for zelda narrow using the 'get_action' method and the '# Interact with the environment and create a video'\n",
    "# section in the 101_train-decision-transformers.ipynb notebook to the right.\n",
    "\n",
    "# 2) build the proper methods in inference to convert to and from states/actions\n",
    "# so the interaction between pcgrl_env and dt_model (i.e. floats for states vs. ohs for pcgrl_env)\n",
    "# is seamless\n",
    "\n",
    "# 3) Run inference via feeding the model the first state from  observation from raw_dataset_data.npz \n",
    "# and confirm that it can build a solveable level!\n",
    "\n",
    "# Function that gets an action from the model using autoregressive prediction with a window of the previous 20 timesteps.\n",
    "def get_action(model, states, actions, rewards, returns_to_go, timesteps):\n",
    "    # This implementation does not condition on past rewards\n",
    "    # model.config.max_length = 100\n",
    "\n",
    "    states = states.reshape(1, -1, model.config.state_dim)\n",
    "    actions = actions.reshape(1, -1, model.config.act_dim)\n",
    "    returns_to_go = returns_to_go.reshape(1, -1, 1)\n",
    "    timesteps = timesteps.reshape(1, -1)\n",
    "\n",
    "    states = states[:, -model.config.max_length :]\n",
    "    actions = actions[:, -model.config.max_length :]\n",
    "    returns_to_go = returns_to_go[:, -model.config.max_length :]\n",
    "    timesteps = timesteps[:, -model.config.max_length :]\n",
    "    padding = model.config.max_length - states.shape[1]\n",
    "    # pad all tokens to sequence length\n",
    "    attention_mask = torch.cat([torch.zeros(padding), torch.ones(states.shape[1])])\n",
    "    attention_mask = attention_mask.to(dtype=torch.long).reshape(1, -1)\n",
    "    states = torch.cat([torch.zeros((1, padding, model.config.state_dim)), states], dim=1).float()\n",
    "    actions = torch.cat([torch.zeros((1, padding, model.config.act_dim)), actions], dim=1).float()\n",
    "    returns_to_go = torch.cat([torch.zeros((1, padding, 1)), returns_to_go], dim=1).float()\n",
    "    timesteps = torch.cat([torch.zeros((1, padding), dtype=torch.long), timesteps], dim=1)\n",
    "\n",
    "    state_preds, action_preds, return_preds = model.original_forward(\n",
    "        states=states,\n",
    "        actions=actions,\n",
    "        rewards=rewards,\n",
    "        returns_to_go=returns_to_go,\n",
    "        timesteps=timesteps,\n",
    "        attention_mask=attention_mask,\n",
    "        return_dict=False,\n",
    "    )\n",
    "\n",
    "    return action_preds[0, -1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd8d0ae-05aa-4e45-bc9a-d867b250494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_state(obs, x, y, obs_size):\n",
    "    map = obs\n",
    "    size = obs_size\n",
    "    pad = obs_size // 2\n",
    "    padded = np.pad(map, pad, constant_values=1)\n",
    "    cropped = padded[y: y + size, x: x + size]\n",
    "    \n",
    "    return cropped\n",
    "\n",
    "\n",
    "def show_state(env, step=0, changes=0, total_reward=0, name=\"\"):\n",
    "    fig = plt.figure(10)\n",
    "    plt.clf()\n",
    "    plt.title(\"{} | Step: {} Changes: {} Total Reward: {}\".format(name, step, changes, total_reward))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "    \n",
    "    \n",
    "model = TrainableDT.from_pretrained(\"/home/jupyter-msiper/bootstrapping-pcgrl/output/checkpoint-2000\")\n",
    "model.eval()\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "from gym_pcgrl.envs.pcgrl_env import PcgrlEnv\n",
    "from utils import make_vec_envs as mkvenv\n",
    "\n",
    "\n",
    "max_ep_len = 1000\n",
    "device = \"cpu\"\n",
    "scale = 1.0 #1000.0  # normalization for rewards/returns\n",
    "TARGET_RETURN = 87 / scale  # evaluation is conditioned on a return of 12000, scaled accordingly\n",
    "\n",
    "state_mean = collator.state_mean.astype(np.float32)\n",
    "state_std = collator.state_std.astype(np.float32)\n",
    "# print(state_mean)\n",
    "\n",
    "state_dim = 484#env.observation_space.shape[0]\n",
    "act_dim = 8#env.action_space.shape[0]\n",
    "\n",
    "\n",
    "state_mean = torch.from_numpy(state_mean).to(device=device)\n",
    "state_std = torch.from_numpy(state_std).to(device=device)\n",
    "\n",
    "\n",
    "def infer(model, trial_num, total_solved):\n",
    "    # Interact with 2085 environment and create a video\n",
    "    seen_levels = [int(ind) for ind in open(\"/home/jupyter-msiper/bootstrapping-pcgrl/inds_used.txt\", \"r\").readlines()]\n",
    "    EPISODE_INDEX = random.choice(seen_levels)\n",
    "    print(f\"Epsiode start: {EPISODE_INDEX}\")\n",
    "    episode_return, episode_length = 0, 0\n",
    "    env = PcgrlEnv(prob=\"zelda\", rep=\"narrow\")\n",
    "    state = env.reset()\n",
    "    # env._rep._map = (dataset[EPISODE_INDEX]['observations'][0]*8.0).astype(int).reshape(22,22)[10:10+7, 10:10+11]\n",
    "    \n",
    "    start_state = env.reset()\n",
    "    # data['obs']\n",
    "    # print(state)\n",
    "    target_return = torch.tensor(TARGET_RETURN, device=device, dtype=torch.float32).reshape(1, 1)\n",
    "    states = torch.from_numpy(transform_state(start_state['map'], env._rep._x, env._rep._y, 22)/8.0).reshape(1, state_dim).to(device=device, dtype=torch.float32)\n",
    "    # states = torch.from_numpy(start_state['map']).to(device=device, dtype=torch.float32)\n",
    "    actions = torch.zeros((0, act_dim), device=device, dtype=torch.float32)\n",
    "    rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
    "    print(f\"max_ep_len: {max_ep_len}\")\n",
    "    done = solved = False\n",
    "    t = 0\n",
    "    timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
    "    while not done or not solved:\n",
    "        actions = torch.cat([actions, torch.zeros((1, act_dim), device=device)], dim=0)\n",
    "        rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n",
    "\n",
    "        action = get_action(\n",
    "            model,\n",
    "            states, #(states - state_mean) / state_std,\n",
    "            actions,\n",
    "            rewards,\n",
    "            target_return,\n",
    "            timesteps,\n",
    "        )\n",
    "        actions[-1] = action\n",
    "        action = action.detach().cpu().numpy()\n",
    "        action = np.argmax(action) + 1\n",
    "        # print(f\"action: {action}\")\n",
    "\n",
    "        state, reward, done, info = env.step(action)\n",
    "        show_state(env, step=0, changes=0, total_reward=0, name=f\"Trial: {trial_num}, num_solved: {total_solved}\")\n",
    "        actions[-1] = action / 8.0\n",
    "        solved = info['solved']\n",
    "\n",
    "\n",
    "        cur_state = torch.from_numpy(transform_state(state['map'], env._rep._x, env._rep._y, 22)/8.0).to(device=device).reshape(1, state_dim)\n",
    "        states = torch.cat([states, cur_state], dim=0)\n",
    "        rewards[-1] = reward\n",
    "\n",
    "        pred_return = target_return[0, -1] - (reward / scale)\n",
    "        target_return = torch.cat([target_return, pred_return.reshape(1, 1)], dim=1)\n",
    "        timesteps = torch.cat([timesteps, torch.ones((1, 1), device=device, dtype=torch.long) * (t + 1)], dim=1)\n",
    "\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "        print(f\"episode_return: {episode_return}, solved: {solved}, done: {done}\")\n",
    "        t += 1\n",
    "\n",
    "        if done or solved or t >= 350:\n",
    "            # input('')\n",
    "            # env.reset()\n",
    "            # env._rep._map = (dataset[EPISODE_INDEX]['observations'][0]*8.0).astype(int).reshape(22,22)[10:10+7, 10:10+11]\n",
    "            if solved:\n",
    "                # input('')\n",
    "                return True\n",
    "                \n",
    "            else:\n",
    "                return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15cd5c6-1008-45cb-aa4f-e0554ee87cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next steps:\n",
    "# I want to append multiple episodes to the same trajectory to confirm I can train large dataset with the DataCollator object\n",
    "# Test with using the cumulative reward from the PoD trajectory\n",
    "# env = PcgrlEnv(prob=\"zelda\", rep=\"narrow\")\n",
    "NUM_TRIALS = 500\n",
    "num_solved = 0\n",
    "for t_i in range(NUM_TRIALS):\n",
    "    level_solved = infer(model, t_i, num_solved)\n",
    "    num_solved += int(level_solved)\n",
    "    \n",
    "print(f\"Pct solved: {num_solved/float(NUM_TRIALS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5dd1e1-e173-4879-a2ff-494fc1ae16a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs_pcgrl_py_310",
   "language": "python",
   "name": "bs_pcgrl_py_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
